{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8898826,"sourceType":"datasetVersion","datasetId":5328030}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyelim\n!pip install ultralytics torchmetrics -q\n\nimport os\nimport yaml\nimport random\nimport cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nos.system('pip install pycocotools')\n\n# Gerekli importlarÄ± tazeleyelim\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport torch\nfrom PIL import Image\n\n# PyTorch ve Model KÃ¼tÃ¼phaneleri\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, ssdlite320_mobilenet_v3_large, retinanet_resnet50_fpn_v2\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.retinanet import RetinaNetHead\nfrom torch.optim import SGD\nfrom ultralytics import YOLO\n\n# Metrikler\ntry:\n    from torchmetrics.detection.mean_ap import MeanAveragePrecision\nexcept ImportError:\n    print(\"Torchmetrics yÃ¼klenemedi, metrik hesaplamasÄ± Ã§alÄ±ÅŸmayabilir.\")\n\n# Cihaz ayarÄ± (GPU varsa kullan)\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f\"âœ… Ã‡alÄ±ÅŸma ortamÄ± hazÄ±r. Cihaz: {DEVICE}\")","metadata":{"_uuid":"b36d3b6b-932b-4dea-b174-86adacb9a02d","_cell_guid":"059fcab9-0b2c-4747-a8ce-ec83db54d37b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- AYARLAR ---\nEPOCHS =5       # Ã–dev isterine gÃ¶re 100 olmalÄ± (Test iÃ§in 5-10 yapabilirsin)\nBATCH_SIZE = 4        # GPU hafÄ±zasÄ± yetmezse 2'ye dÃ¼ÅŸÃ¼r\nIMG_SIZE = 320      # GÃ¶rÃ¼ntÃ¼ boyutu\nNUM_CLASSES_PT = 2    # PyTorch iÃ§in: 0=Background, 1=Person\n\n# --- VERÄ° YOLU BULMA ---\nstart_dir = \"/kaggle/input/thermal-image-people-detection/People Detection - Themalt\"\nroot_dir = \"/kaggle/input/thermal-image-people-detection/People Detection - Themal\"\n\nprint(\"ğŸ•µï¸â€â™‚ï¸ Veri seti aranÄ±yor...\")\nfor dirpath, dirnames, filenames in os.walk(start_dir):\n    if \"train\" in dirnames and \"valid\" in dirnames:\n        root_dir = dirpath\n        break\n\nif root_dir:\n    train_path = os.path.join(root_dir, \"train\")\n    valid_path = os.path.join(root_dir, \"valid\")\n    print(f\"âœ… Veri seti bulundu: {root_dir}\")\nelse:\n    print(\"âŒ HATA: Veri seti bulunamadÄ±! LÃ¼tfen Kaggle saÄŸ menÃ¼den veri setini eklediÄŸine emin ol.\")\n\n# --- YOLO Ä°Ã‡Ä°N YAML OLUÅTURMA ---\nif root_dir:\n    data_yaml = {\n        'train': os.path.join(train_path, \"images\"),\n        'val': os.path.join(valid_path, \"images\"),\n        'nc': 1,\n        'names': ['person']\n    }\n    with open('dataset.yaml', 'w') as f:\n        yaml.dump(data_yaml, f)\n    print(\"âœ… YOLO iÃ§in 'dataset.yaml' oluÅŸturuldu.\")","metadata":{"_uuid":"11e16cf0-4a0f-45ab-ad50-8c6538a5b811","_cell_guid":"0f907bba-5020-4849-ad23-7591df97c80d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ThermalDataset(Dataset):\n    def __init__(self, root_dir, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        self.image_dir = os.path.join(root_dir, \"images\")\n        self.label_dir = os.path.join(root_dir, \"labels\")\n        self.imgs = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n\n        # Resmi oku ve RGB yap\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w, _ = img.shape\n        img_pil = Image.fromarray(img)\n\n        boxes = []\n        labels = []\n\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                lines = f.readlines()\n                for line in lines:\n                    parts = list(map(float, line.strip().split()))\n                    if len(parts) == 5:\n                        cls, cx, cy, bw, bh = parts\n                        # YOLO formatÄ±ndan (normalize) -> Pascal VOC formatÄ±na (piksel) Ã§evir\n                        x_min = (cx - bw/2) * w\n                        y_min = (cy - bh/2) * h\n                        x_max = (cx + bw/2) * w\n                        y_max = (cy + bh/2) * h\n\n                        # HatalÄ± kutularÄ± engelle\n                        if x_max > x_min and y_max > y_min:\n                            boxes.append([x_min, y_min, x_max, y_max])\n                            labels.append(1) # 1: Person\n\n        # TensÃ¶rlere Ã§evir\n        target = {}\n        if len(boxes) > 0:\n            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n            target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n            target[\"iscrowd\"] = torch.zeros((len(labels),), dtype=torch.int64)\n            target[\"image_id\"] = torch.tensor([idx])\n        else:\n            # Kutu yoksa boÅŸ tensÃ¶r gÃ¶nder\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n            target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n            target[\"image_id\"] = torch.tensor([idx])\n\n        if self.transforms:\n            from torchvision import transforms as T\n            # Sadece ToTensor uyguluyoruz, gerekirse augmentasyon eklenebilir\n            t = T.Compose([T.ToTensor()])\n            img_tensor = t(img_pil)\n        \n        return img_tensor, target\n\n    def __len__(self):\n        return len(self.imgs)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Datasetleri OluÅŸtur\ntrain_dataset = ThermalDataset(train_path, transforms=True)\nvalid_dataset = ThermalDataset(valid_path, transforms=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n\nprint(f\"âœ… Dataset HazÄ±r. Train: {len(train_dataset)}, Valid: {len(valid_dataset)} gÃ¶rsel.\")","metadata":{"_uuid":"17a158f7-d9d6-4acc-bfb8-3bca2886b9d4","_cell_guid":"df3bfbb5-57a9-42d6-851f-d74164d3003a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_sample(loader):\n    images, targets = next(iter(loader))\n    img = images[0].permute(1, 2, 0).cpu().numpy().copy()\n    boxes = targets[0]['boxes'].cpu().numpy()\n    \n    plt.figure(figsize=(10,8))\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box.astype(int)\n        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n    \n    plt.imshow(img)\n    plt.title(\"Ã–rnek Veri KontrolÃ¼\")\n    plt.axis('off')\n    plt.show()\n\ntry:\n    visualize_sample(train_loader)\nexcept Exception as e:\n    print(f\"GÃ¶rselleÅŸtirme hatasÄ±: {e}\")","metadata":{"_uuid":"fee11acf-0ec3-49f4-b1ed-b350e08b4903","_cell_guid":"5ebafc45-a945-49ad-8592-2aba8ee01b7e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, loader, epochs, name):\n    print(f\"\\nğŸš€ [{name}] EÄŸitimi BaÅŸlÄ±yor... ({epochs} Epoch)\")\n    model.to(DEVICE)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n    \n    model.train()\n    loss_history = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        batch_count = 0\n        \n        for images, targets in loader:\n            images = [img.to(DEVICE) for img in images]\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            \n            epoch_loss += losses.item()\n            batch_count += 1\n        \n        avg_loss = epoch_loss/batch_count\n        loss_history.append(avg_loss)\n        \n        if (epoch+1) % 10 == 0 or epoch == 0:\n            print(f\"   Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n            \n    print(f\"âœ… {name} EÄŸitimi TamamlandÄ±.\")\n    return model, loss_history","metadata":{"_uuid":"baa120de-0c79-4c90-b668-46847782980e","_cell_guid":"b4e7d8d2-5712-4cd4-ac51-606275e7c565","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== MODEL 1: YOLOv8m EÄŸitimi ===\")\n\nmodel_yolo = YOLO('yolov8m.pt') \n\nresults_yolo = model_yolo.train(\n    data='dataset.yaml',\n    epochs=EPOCHS,\n    imgsz=IMG_SIZE,\n    batch=8, # YOLO genelde daha optimize Ã§alÄ±ÅŸÄ±r, batch'i artÄ±rabilirsin\n    name='yolo_thermal_project',\n    verbose=False\n)\n\nprint(\"YOLO eÄŸitimi bitti.\")","metadata":{"_uuid":"10eaff0f-5e78-40aa-91fa-50cb80c8d270","_cell_guid":"8d928c7d-5c2d-4223-a3a3-abd275baa183","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== MODEL 2: Faster R-CNN (ResNet-50) EÄŸitimi ===\")\n\n# Modeli YÃ¼kle\nmodel_frcnn = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n\n# Ã‡Ä±kÄ±ÅŸ katmanÄ±nÄ± (Head) deÄŸiÅŸtir (2 SÄ±nÄ±f: Background + Person)\nin_features = model_frcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_frcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES_PT)\n\n# EÄŸit\ntrained_frcnn, loss_frcnn = train_model(model_frcnn, train_loader, EPOCHS, \"Faster R-CNN\")","metadata":{"_uuid":"6533c2ef-ca98-431f-96a4-726db2ea51f9","_cell_guid":"0763425a-ac07-4ad2-932a-69f0dc87953a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== MODEL 3: RetinaNet EÄŸitimi ===\")\n\n# Modeli YÃ¼kle\nmodel_retina = retinanet_resnet50_fpn_v2(weights=\"DEFAULT\")\n\n# Head katmanÄ±nÄ± deÄŸiÅŸtir\nnum_anchors = model_retina.head.classification_head.num_anchors\nmodel_retina.head = RetinaNetHead(in_channels=256, num_anchors=num_anchors, num_classes=NUM_CLASSES_PT)\n\n# EÄŸit (EÄŸer CUDA hatasÄ± alÄ±rsan batch size'Ä± dÃ¼ÅŸÃ¼r)\ntrained_retina, loss_retina = train_model(model_retina, train_loader, EPOCHS, \"RetinaNet\")","metadata":{"_uuid":"7d3af1d3-6d5d-4377-a64a-3cf967919af3","_cell_guid":"76bd7905-30e3-42fa-8a05-f9aa26fa7ccb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== MODEL 4: SSDLite (MobileNetV3) EÄŸitimi ===\")\n\n# 1. Pretrained aÄŸÄ±rlÄ±klarÄ± al\nbase_model = ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\npretrained_dict = base_model.state_dict()\n\n# 2. Yeni boÅŸ model oluÅŸtur (Bizim sÄ±nÄ±f sayÄ±mÄ±zla)\nmodel_ssd = ssdlite320_mobilenet_v3_large(weights=None, num_classes=NUM_CLASSES_PT)\nmodel_dict = model_ssd.state_dict()\n\n# 3. Sadece uyuÅŸan katmanlarÄ± kopyala (Head hariÃ§)\nfiltered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\nmodel_dict.update(filtered_dict)\nmodel_ssd.load_state_dict(model_dict)\n\nprint(\"âœ… SSDLite aÄŸÄ±rlÄ±klarÄ± baÅŸarÄ±yla aktarÄ±ldÄ±.\")\n\n# EÄŸit\ntrained_ssd, loss_ssd = train_model(model_ssd, train_loader, EPOCHS, \"SSDLite\")","metadata":{"_uuid":"c9728439-e6eb-4fee-8e82-091008201ead","_cell_guid":"f2e234d3-ff08-402e-9efb-243d5b179c9b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_pytorch(model, loader, device):\n    print(f\"âš™ï¸ DeÄŸerlendirme yapÄ±lÄ±yor... (Cihaz: {device})\")\n    model.eval()\n    metric = MeanAveragePrecision(iou_type=\"bbox\")\n    \n    # Ä°lerleme durumunu gÃ¶rmek iÃ§in basit sayaÃ§\n    batch_index = 0\n    total_batches = len(loader)\n    \n    with torch.no_grad():\n        for images, targets in loader:\n            images = [img.to(device) for img in images]\n            preds = model(images)\n            \n            # CPU'ya alÄ±p formata uygun hale getir\n            preds_cpu = [{k: v.to(\"cpu\") for k, v in p.items()} for p in preds]\n            targets_cpu = [{k: v.to(\"cpu\") for k, v in t.items()} for t in targets]\n            \n            metric.update(preds_cpu, targets_cpu)\n            \n            batch_index += 1\n            if batch_index % 10 == 0:\n                print(f\"   Batch {batch_index}/{total_batches} iÅŸlendi.\")\n            \n    print(\"ğŸ“ˆ Metrikler hesaplanÄ±yor...\")\n    result = metric.compute()\n    return result['map_50'].item(), result['map'].item()\n\n# --- FÄ°NAL HESAPLAMA ---\nprint(\"\\nğŸ“Š --- FÄ°NAL SONUÃ‡LARI HESAPLANIYOR ---\")\n\n# 1. YOLO SonuÃ§larÄ± (Zaten hazÄ±rdÄ±)\n# Not: EÄŸer results_yolo nesnesi tensÃ¶r ise .item() gerekebilir, deÄŸilse direkt alÄ±nÄ±r.\ntry:\n    yolo_map50 = results_yolo.box.map50\n    yolo_map = results_yolo.box.map\nexcept:\n    yolo_map50 = 0.0\n    yolo_map = 0.0\n\n# 2. PyTorch Modelleri\nprint(\"\\n--- Faster R-CNN DeÄŸerlendiriliyor ---\")\nfrcnn_map50, frcnn_map = evaluate_pytorch(trained_frcnn, valid_loader, DEVICE)\n\nprint(\"\\n--- RetinaNet DeÄŸerlendiriliyor ---\")\nretina_map50, retina_map = evaluate_pytorch(trained_retina, valid_loader, DEVICE)\n\nprint(\"\\n--- SSDLite DeÄŸerlendiriliyor ---\")\nssd_map50, ssd_map = evaluate_pytorch(trained_ssd, valid_loader, DEVICE)\n\n# Tabloyu YazdÄ±r\nprint(\"\\n\" + \"=\"*65)\nprint(f\"| {'MODEL ADI':<20} | {'mAP@0.50 (DoÄŸruluk)':<20} | {'mAP@0.50:0.95':<15} |\")\nprint(f\"|{'-'*22}|{'-'*22}|{'-'*17}|\")\nprint(f\"| {'YOLOv8m':<20} | {yolo_map50:.4f}               | {yolo_map:.4f}          |\")\nprint(f\"| {'Faster R-CNN':<20} | {frcnn_map50:.4f}               | {frcnn_map:.4f}          |\")\nprint(f\"| {'RetinaNet':<20} | {retina_map50:.4f}               | {retina_map:.4f}          |\")\nprint(f\"| {'SSDLite':<20} | {ssd_map50:.4f}               | {ssd_map:.4f}          |\")\nprint(\"=\"*65)","metadata":{"_uuid":"acbb30b4-6708-48b4-9037-371e4b7947ea","_cell_guid":"d9f32c6a-574a-4da2-97fe-a57f0c10060c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.cuda.empty_cache()\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T21:43:55.631563Z","iopub.execute_input":"2026-01-10T21:43:55.631871Z","iopub.status.idle":"2026-01-10T21:44:02.626513Z","shell.execute_reply.started":"2026-01-10T21:43:55.631831Z","shell.execute_reply":"2026-01-10T21:44:02.625659Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection import retinanet_resnet50_fpn\n\nnum_classes = 2  # arka plan + 1 sÄ±nÄ±f (Ã¶rnek)\n\nmodel_frcnn = fasterrcnn_resnet50_fpn(num_classes=num_classes)\nmodel_retina = retinanet_resnet50_fpn(num_classes=num_classes)\n\nmodel_frcnn.to(DEVICE)\nmodel_retina.to(DEVICE)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = list(model_frcnn.parameters()) + list(model_retina.parameters())\n\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.0005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=1,          # ğŸ”´ RAM koruma\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    model_frcnn.train()\n    model_retina.train()\n\n    total_loss_epoch = 0\n\n    for images, targets in train_loader:\n        images = [img.to(DEVICE) for img in images]\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        # ğŸ”¹ Faster R-CNN loss\n        loss_dict_frcnn = model_frcnn(images, targets)\n        loss_frcnn = sum(loss for loss in loss_dict_frcnn.values())\n\n        # ğŸ”¹ RetinaNet loss\n        loss_dict_retina = model_retina(images, targets)\n        loss_retina = sum(loss for loss in loss_dict_retina.values())\n\n        # ğŸ”¥ ORTAK LOSS\n        total_loss = loss_frcnn + loss_retina\n\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        total_loss_epoch += total_loss.item()\n\n    print(\n        f\"[Epoch {epoch+1}/{EPOCHS}] \"\n        f\"FRCNN Loss: {loss_frcnn.item():.4f} | \"\n        f\"Retina Loss: {loss_retina.item():.4f}\"\n    )\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# FINE-TUNING (DERÄ°N Ã–ÄRENME - GÃœVENLÄ° MOD)\n# =====================================\nimport gc\n\nprint(\"ğŸ”§ Fine-Tuning BaÅŸladÄ± (Faster R-CNN)\")\n\n# 1ï¸âƒ£ Bellek TemizliÄŸi: Ã–nceki hatalardan kalan RAM yÃ¼kÃ¼nÃ¼ boÅŸaltalÄ±m\ntorch.cuda.empty_cache()\ngc.collect()\n\n# 2ï¸âƒ£ Veri YÃ¼kleyiciyi Bellek Dostu YapalÄ±m (num_workers=0 hatayÄ± Ã§Ã¶zer)\ntrain_loader_fine = DataLoader(\n    train_dataset, \n    batch_size=2,          # RAM kullanÄ±mÄ± iÃ§in batch'i 2'ye dÃ¼ÅŸÃ¼rdÃ¼k\n    shuffle=True, \n    collate_fn=collate_fn, \n    num_workers=0          # \"Killed\" hatasÄ±nÄ± engelleyen kritik ayar\n)\n\n# 3ï¸âƒ£ Katman AyarlarÄ±\n# Backbone'u dondur (Genel Ã¶zellikleri koru)\nfor param in trained_frcnn.backbone.parameters():\n    param.requires_grad = False\n\n# Sadece son katman bloÄŸunu (layer4) ve kafayÄ± (head) eÄŸitmek istersen (daha derin ince ayar):\nfor param in trained_frcnn.backbone.body.layer4.parameters():\n    param.requires_grad = True\n\n# 4ï¸âƒ£ Optimizer ve Parametreler\ntrainable_params = [p for p in trained_frcnn.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    trainable_params,\n    lr=0.0001,      # Ã‡ok kÃ¼Ã§Ã¼k bir LR: Mevcut bilgileri bozmadan Ã¼zerine ekler\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\nEPOCHS_FINE = 15\ntrained_frcnn.to(DEVICE)\ntrained_frcnn.train()\n\n# 5ï¸âƒ£ EÄŸitim DÃ¶ngÃ¼sÃ¼\ntry:\n    for epoch in range(EPOCHS_FINE):\n        total_loss = 0\n        batch_count = 0\n\n        for images, targets in train_loader_fine:\n            images = [img.to(DEVICE) for img in images]\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n            loss_dict = trained_frcnn(images, targets)\n            loss = sum(l for l in loss_dict.values())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            batch_count += 1\n\n        print(f\"[Fine-Tuning] Epoch {epoch+1}/{EPOCHS_FINE} | Ortalama Loss: {total_loss/batch_count:.4f}\")\n        \nexcept Exception as e:\n    print(f\"âš ï¸ EÄŸitim sÄ±rasÄ±nda bir hata oluÅŸtu: {e}\")\n    print(\"Ä°pucu: EÄŸer hala 'Killed' alÄ±yorsan batch_size=1 yapabilirsin.\")\n\nprint(\"âœ… Fine-Tuning tamamlandÄ±.\")\n\n\n\n\ndef ensemble_detector(models, image, device, iou_thr=0.5):\n    boxes_all, scores_all, model_index = [], [], []\n\n    for i, model in enumerate(models):\n        model.to(device) # Modelin GPU'da olduÄŸundan emin ol\n        model.eval()\n        with torch.no_grad():\n            # [image] formatÄ± PyTorch modelleri iÃ§in doÄŸrudur (0-1 arasÄ± float tensor)\n            output = model([image.to(device)])[0]\n\n        # Sadece belirli bir gÃ¼ven eÅŸiÄŸinin Ã¼zerindeki kutularÄ± alalÄ±m (Ã–rn: 0.5)\n        # Bu, baskÄ±nlÄ±k analizinin daha anlamlÄ± olmasÄ±nÄ± saÄŸlar.\n        keep = output[\"scores\"] > 0.4 \n        \n        if len(output[\"boxes\"][keep]) > 0:\n            boxes_all.append(output[\"boxes\"][keep].cpu())\n            scores_all.append(output[\"scores\"][keep].cpu())\n            model_index += [i] * len(output[\"boxes\"][keep])\n\n    if len(boxes_all) == 0:\n        return torch.tensor([]), torch.tensor([]), []\n\n    boxes_all = torch.cat(boxes_all)\n    scores_all = torch.cat(scores_all)\n\n    selected = []\n    used = set()\n\n    # Skorlara gÃ¶re sÄ±ralayalÄ±m ki en iyi kutudan baÅŸlayalÄ±m (NMS mantÄ±ÄŸÄ±)\n    indices = torch.argsort(scores_all, descending=True)\n\n    for i in indices:\n        idx = i.item()\n        if idx in used:\n            continue\n\n        overlaps = box_iou(boxes_all[idx].unsqueeze(0), boxes_all)[0]\n        group = torch.where(overlaps > iou_thr)[0]\n\n        selected.append(idx)\n        used.update(group.tolist())\n\n    return boxes_all[selected], scores_all[selected], [model_index[i] for i in selected]\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# MODEL DOMINANCE ANALYSIS\n# =====================================\n\nmodels = [trained_frcnn, trained_retina, trained_ssd]\nmodel_names = [\"Faster R-CNN\", \"RetinaNet\", \"SSDLite\"]\n\ndominance = {name: 0 for name in model_names}\n\nfor images, _ in valid_loader:\n    image = images[0]\n    _, _, model_ids = ensemble_detector(models, image, DEVICE)\n\n    for mid in model_ids:\n        dominance[model_names[mid]] += 1\n\nprint(\"\\nğŸ“Š MODEL BASKINLIK ANALÄ°ZÄ°\")\nprint(\"=\"*35)\nfor k, v in dominance.items():\n    print(f\"{k:<15}: {v}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}